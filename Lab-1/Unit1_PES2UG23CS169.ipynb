{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0a5a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (5.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dhany\\appdata\\roaming\\python\\python313\\site-packages (from typer-slim->transformers) (8.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d756cd",
   "metadata": {},
   "source": [
    "## Experiment 1: Text Generation\n",
    "\n",
    "Task: Try to generate text using the prompt: \"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532b4702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590e49a821d4468681653bfab69b52a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is. thele [ [ [ [ [ \" \" ( \" \". it it it it it it it it it actually so and and and and are as..................... : you many so austin two to \". it it it it or and and and and ( \". it it it it it it it it or so jefferson in the and. it it it it it it it it it it org, \" he bar lot, ( \". the an an a mrs.. it it it it was belle. him and and and \" ( \" ( ) ( \" ( \" ( ( - ) ( ) a a scheme on many way way way way re legal i an an an / the the ring. are were from \".. the the those being being which as our their to the women it to he to is\\'\\'\\'\" ( ) ( \" (\\') of with me is her of common just hers (. it or some some some some some some some mostly rock many so jefferson lu,..........................................................................................................................................'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_bert = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "generator_bert(\"The future of Artificial Intelligence is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85148313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e561efa327c147109f606673312c5858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_roberta = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"roberta-base\"\n",
    ")\n",
    "\n",
    "generator_roberta(\"The future of Artificial Intelligence is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606b0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4299baac6b4496c80ee268420e13187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence isayingDayayinggian beansiesta Tomato Tomato Tomato+) schematicaying REG beans beans beans greeting greeting Mininggianfacing sentiments beans beans organization leveraging leveragingElryceryce leveraging chick)[olesc Tomato intense census greeting organization leveragingDiamondeah twistolesc chairmanDiamond)[ beans Surve organization)[)[ beans beans visionary Armenia rec Yen REGGate greetinglo exercisesDiamondDiamond+) misrepresent chairman chairman chairman exercisesbett)[F recPersIGHTApple+) Kare+) chairman beans 398)[ interfaces)[)[)[IGHT rec+) chairman twist beansbett greeting greeting rec)[)[+) organizationIGHT rec chairman)[)[Ébett)[)[ organization)[IGHTIGHTIGHT Kare+) Kare misrepresent+) organization greeting organization+)É)[)[Flash Elastic)[IGHTPersIGHT 13olesc)[ Nicola)[)[ DruIGHT)[ organization interfaces organization)[ beansPersDiamond)[)[ BarthIGHTPers+) leveragingIGHTPers)[)[ occupiesPers ArmeniaIGHT wattsbettIGHTIGHT interfaces interfaces redesigned)[+)bett chairman)[PersIGHTIGHT+)IGHT 398IGHTIGHT DuffyPers 13 Pyrrha anniversary)[IGHTbettDiamondhub Surve)[ ordinancesIGHTIGHT misrepresentIGHT prophet)[IGHTifiers SurveDiamondDiamondLeaksWbett confidently Surve)[IGHT SurveIGHTIGHT FrankensteinIGHT greetingbettbettIGHTbett+) watts)[IGHT interfaces 13)[)[ watts ClubIGHTbett greetingIGHTDiamond+))[IGHT)[ interfacesIGHTIGHTbettbedroomDiamond 13Gate 398'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_bart = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"facebook/bart-base\"\n",
    ")\n",
    "\n",
    "generator_bart(\n",
    "    \"The future of Artificial Intelligence is\",\n",
    "    max_length=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7953ea",
   "metadata": {},
   "source": [
    "### Observation – Text Generation\n",
    "\n",
    "- BERT generated incoherent and repetitive text when forced to perform text generation.\n",
    "- RoBERTa also failed to generate meaningful text.\n",
    "- BART was able to generate a short and somewhat coherent continuation.\n",
    "- This shows that encoder-only models struggle with text generation, while encoder-decoder models perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c571d5",
   "metadata": {},
   "source": [
    "## Experiment 2: Masked Language Modeling (Missing Word)\n",
    "\n",
    "Task: Predict the missing word in: \"The goal of Generative AI is to [MASK] new content.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b3e752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28438a718184482bb6a4393ab92b414d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.539692759513855,\n",
       "  'token': 3443,\n",
       "  'token_str': 'create',\n",
       "  'sequence': 'the goal of generative ai is to create new content.'},\n",
       " {'score': 0.15575766563415527,\n",
       "  'token': 9699,\n",
       "  'token_str': 'generate',\n",
       "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
       " {'score': 0.05405496060848236,\n",
       "  'token': 3965,\n",
       "  'token_str': 'produce',\n",
       "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
       " {'score': 0.044515229761600494,\n",
       "  'token': 4503,\n",
       "  'token_str': 'develop',\n",
       "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
       " {'score': 0.017577484250068665,\n",
       "  'token': 5587,\n",
       "  'token_str': 'add',\n",
       "  'sequence': 'the goal of generative ai is to add new content.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_bert = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "fill_mask_bert(\"The goal of Generative AI is to [MASK] new content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384a4978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7df02c65e7842768b8a2bac9a4d3e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.37113118171691895,\n",
       "  'token': 5368,\n",
       "  'token_str': ' generate',\n",
       "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
       " {'score': 0.3677138090133667,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.08351466804742813,\n",
       "  'token': 8286,\n",
       "  'token_str': ' discover',\n",
       "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
       " {'score': 0.02133519947528839,\n",
       "  'token': 465,\n",
       "  'token_str': ' find',\n",
       "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
       " {'score': 0.01652175933122635,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_roberta = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"roberta-base\"\n",
    ")\n",
    "\n",
    "fill_mask_roberta(\"The goal of Generative AI is to <mask> new content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "730fb69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e19466a5a0949a3929aba915f271562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07461544126272202,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.06571853160858154,\n",
       "  'token': 244,\n",
       "  'token_str': ' help',\n",
       "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
       " {'score': 0.060880184173583984,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
       " {'score': 0.035935722291469574,\n",
       "  'token': 3155,\n",
       "  'token_str': ' enable',\n",
       "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
       " {'score': 0.03319481760263443,\n",
       "  'token': 1477,\n",
       "  'token_str': ' improve',\n",
       "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_bart = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"facebook/bart-base\"\n",
    ")\n",
    "\n",
    "fill_mask_bart(\"The goal of Generative AI is to <mask> new content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cc074",
   "metadata": {},
   "source": [
    "### Observation – Fill Mask\n",
    "\n",
    "- BERT accurately predicted meaningful words such as \"generate\" and \"create\".\n",
    "- RoBERTa produced correct and contextually relevant predictions.\n",
    "- BART was able to fill the mask but with slightly less accurate predictions.\n",
    "- Masked Language Modeling works best for models explicitly trained for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351028a9",
   "metadata": {},
   "source": [
    "## Experiment 3: Question Answering\n",
    "\n",
    "Task: Answer the question \"What are the risks?\" based on the context: \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10f7018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be24505d4d6d414ea4c214cbcf1714d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.012824046425521374,\n",
       " 'start': 46,\n",
       " 'end': 66,\n",
       " 'answer': 'hallucinations, bias'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "qa_bert(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ca2fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a1acf5096d44418efbe3bca15778c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.00978428591042757,\n",
       " 'start': 0,\n",
       " 'end': 31,\n",
       " 'answer': 'Generative AI poses significant'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_roberta = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"roberta-base\"\n",
    ")\n",
    "\n",
    "qa_roberta(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c5070a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff99949993394ac2bee3bc0d18cf8e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.weight | MISSING | \n",
      "qa_outputs.bias   | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.012210825458168983,\n",
       " 'start': 0,\n",
       " 'end': 31,\n",
       " 'answer': 'Generative AI poses significant'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bart = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"facebook/bart-base\"\n",
    ")\n",
    "\n",
    "qa_bart(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78d9f4",
   "metadata": {},
   "source": [
    "### Observation – Question Answering\n",
    "\n",
    "- All three models produced answers, but the responses were inconsistent.\n",
    "- Some answers were incomplete or had low confidence scores.\n",
    "- Since base models were used without QA fine-tuning, performance was limited.\n",
    "- Encoder strength helped, but lack of task-specific training affected accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d11ec",
   "metadata": {},
   "source": [
    "### Overall Conclusion\n",
    "\n",
    "This experiment highlights the importance of model architecture. Encoder-only models like BERT and RoBERTa perform well on understanding tasks such as masked word prediction but fail at text generation. Encoder-decoder models like BART are better suited for generation tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
