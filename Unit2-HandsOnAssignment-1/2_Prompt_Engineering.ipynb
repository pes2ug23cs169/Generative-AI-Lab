{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pxypuZCG_OB"
      },
      "source": [
        "---\n",
        "\n"
      ],
      "id": "-pxypuZCG_OB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB4uE0qSG_OG"
      },
      "source": [
        "# Unit 2 - Part 2a: The Anatomy of a Prompt\n",
        "\n",
        "## 1. Introduction: Stochasticity (Randomness)\n",
        "\n",
        "Why does the AI give different answers? Because it is **Stochastic** (Random).\n",
        "\n",
        "It predicts the NEXT TOKEN based on probability.\n",
        "\n",
        "### Visualizing the Prediction\n",
        "Input: `\"The sky is...\"`\n",
        "\n",
        "| Word | Probability | Selected? (Temp=0) | Selected? (Temp=1) |\n",
        "|------|-------------|--------------------|--------------------|\n",
        "| Blue | 80% | ✅ | ❌ |\n",
        "| Gray | 15% | ❌ | ✅ |\n",
        "| Green| 1% | ❌ | ❌ |\n",
        "\n",
        "Prompt Engineering is the art of **manipulating these probabilities**."
      ],
      "id": "vB4uE0qSG_OG"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv --upgrade --quiet langchain langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSkWrXiCHHUR",
        "outputId": "dca9348e-1383-4dfe-d546-262f9467dc12"
      },
      "id": "KSkWrXiCHHUR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/500.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1ea88321",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea88321",
        "outputId": "1cfab78a-20a6-406d-efdd-281c94b7223a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "# Using Low Temp for consistent comparison\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
        "# from langchain_groq import ChatGroq\n",
        "\n",
        "# llm = ChatGroq(\n",
        "#     model=\"llama3-8b-8192\",\n",
        "#     temperature=0\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b87b7d",
      "metadata": {
        "id": "79b87b7d"
      },
      "source": [
        "## 2. The CO-STAR Framework (simplified)\n",
        "\n",
        "A good prompt usually has:\n",
        "1.  **C**ontext (Who are you? Who acts?)\n",
        "2.  **O**bjective (What is the task?)\n",
        "3.  **S**tyle (Formal? Funny?)\n",
        "4.  **T**one (Empathetic? Direct?)\n",
        "5.  **A**udience (Who is reading this?)\n",
        "6.  **R**esponse Format (JSON? List?)\n",
        "\n",
        "Let's compare a **Lazy Prompt** vs a **CO-STAR Prompt**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d6811389",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6811389",
        "outputId": "740fcce2-5fd8-48d8-e181-d83e3a27005a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- LAZY PROMPT ---\n",
            "Here are a few options for a rejection email, ranging from a standard template to one for a candidate who interviewed. Choose the one that best fits your situation.\n",
            "\n",
            "---\n",
            "\n",
            "**Option 1: Standard Rejection (No Interview)**\n",
            "\n",
            "This is suitable for candidates who applied but were not selected for an interview.\n",
            "\n",
            "**Subject: Update on Your Application for [Job Title] at [Company Name]**\n",
            "\n",
            "Dear [Candidate Name],\n",
            "\n",
            "Thank you for your interest in the [Job Title] position at [Company Name] and for taking the time to submit your application.\n",
            "\n",
            "We received a large number of highly qualified applications for this role. While your qualifications are impressive, we have decided to move forward with other candidates whose profiles were a closer match for the specific requirements of this position at this time.\n",
            "\n",
            "We appreciate you considering [Company Name] as a potential employer and wish you the best of luck in your job search and future endeavors.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name]\n",
            "[Your Title]\n",
            "[Company Name]\n",
            "[Company Website (Optional)]\n",
            "\n",
            "---\n",
            "\n",
            "**Option 2: Rejection After Interview(s)**\n",
            "\n",
            "This option acknowledges the time and effort the candidate put into the interview process.\n",
            "\n",
            "**Subject: Update Regarding Your Application for [Job Title] at [Company Name]**\n",
            "\n",
            "Dear [Candidate Name],\n",
            "\n",
            "Thank you for your interest in the [Job Title] position at [Company Name] and for taking the time to interview with our team. We enjoyed learning more about your experience and qualifications.\n",
            "\n",
            "We appreciate you sharing your background and insights during our discussions. This was a highly competitive search, and we received applications from many talented individuals. After careful consideration, we have decided to move forward with another candidate whose qualifications and experience were a closer match for the specific needs of this role at this time.\n",
            "\n",
            "We truly appreciate your time and effort throughout the interview process. We wish you the very best in your job search and future career.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name]\n",
            "[Your Title]\n",
            "[Company Name]\n",
            "[Company Website (Optional)]\n",
            "\n",
            "---\n",
            "\n",
            "**Option 3: Rejection with \"Keep on File\" Option (Use with Caution)**\n",
            "\n",
            "Only use this if you genuinely might consider them for future roles and have a system to track this.\n",
            "\n",
            "**Subject: Update on Your Application for [Job Title] at [Company Name]**\n",
            "\n",
            "Dear [Candidate Name],\n",
            "\n",
            "Thank you for your interest in the [Job Title] position at [Company Name] and for taking the time to apply/interview with us. We appreciate you sharing your background and experience.\n",
            "\n",
            "We received a significant number of applications for this role, and the selection process was highly competitive. While your qualifications are impressive, we have decided to move forward with another candidate whose profile was the best fit for our current needs.\n",
            "\n",
            "We were impressed with your [mention something general like \"experience\" or \"enthusiasm\"] and would like to keep your resume on file for future opportunities that may align more closely with your skills.\n",
            "\n",
            "We wish you the best of luck in your job search and future endeavors.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name]\n",
            "[Your Title]\n",
            "[Company Name]\n",
            "[Company Website (Optional)]\n",
            "\n",
            "---\n",
            "\n",
            "**Key Considerations for Rejection Emails:**\n",
            "\n",
            "*   **Be Timely:** Send it as soon as a decision is made. Don't leave candidates hanging.\n",
            "*   **Be Clear and Direct:** Don't beat around the bush, but be polite.\n",
            "*   **Be Professional:** Maintain a positive image for your company.\n",
            "*   **Be Vague on Reasons:** Avoid giving specific reasons for rejection (e.g., \"you lacked X skill,\" \"your personality wasn't a fit\"). This can open the door to legal issues or arguments. Focus on the company's needs and the chosen candidate's \"closer match.\"\n",
            "*   **Personalize:** Always use the candidate's name.\n",
            "*   **Proofread:** Ensure there are no typos or grammatical errors.\n"
          ]
        }
      ],
      "source": [
        "# The Task: Reject a candidate for a job.\n",
        "task = \"Write a rejection email to a candidate.\"\n",
        "\n",
        "print(\"--- LAZY PROMPT ---\")\n",
        "print(llm.invoke(task).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f43fe6",
      "metadata": {
        "id": "48f43fe6"
      },
      "source": [
        "## 3. Hallucination vs. Creativity\n",
        "\n",
        "Did the model make up a reason?\n",
        "Since we didn't give it facts, it **Predicted the most likely reason** (Usually \"Experience\" or \"Volume of applications\").\n",
        "\n",
        "**This is NOT a bug.** It is a feature. The model is *completing the pattern* of a rejection email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "727f0d12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "727f0d12",
        "outputId": "484ac654-241f-4cc6-e434-5042098d3315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STRUCTURED PROMPT ---\n",
            "Hi Bob,\n",
            "\n",
            "Thank you for your interest in RocketBoots. We appreciate your time and effort.\n",
            "\n",
            "While your application was impressive, the requirements for this role have recently changed. We won't be moving forward with your candidacy at this time.\n",
            "\n",
            "Keep flying,\n",
            "RocketBoots HR\n"
          ]
        }
      ],
      "source": [
        "structured_prompt = \"\"\"\n",
        "# Context\n",
        "You are an HR Manager at a quirky startup called 'RocketBoots'.\n",
        "\n",
        "# Objective\n",
        "Write a rejection email to a candidate named Bob.\n",
        "\n",
        "# Constraints\n",
        "1. Be extremely brief (under 50 words).\n",
        "2. Do NOT say 'we found someone better'. Say 'the role changed'.\n",
        "3. Sign off with 'Keep flying'.\n",
        "\n",
        "# Output Format\n",
        "Plain text, no subject line.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- STRUCTURED PROMPT ---\")\n",
        "print(llm.invoke(structured_prompt).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886fa865",
      "metadata": {
        "id": "886fa865"
      },
      "source": [
        "## 4. Key Takeaway: Ambiguity is the Enemy\n",
        "\n",
        "Every piece of information you leave out is a gap the model MUST fill with probability.\n",
        "- If you don't say \"Be brief\", it picks the most probable length (Avg email length).\n",
        "- If you don't say \"Be rude\", it picks the most probable tone (Polite/Neutral)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3478f89a",
      "metadata": {
        "id": "3478f89a"
      },
      "source": [
        "## Assignment\n",
        "\n",
        "Write a structured prompt to generate a **Python Function**.\n",
        "- **Context:** You are a Senior Python Dev.\n",
        "- **Objective:** Write a function to reverse a string.\n",
        "- **Constraint:** It must use recursion (no slicing `[::-1]`).\n",
        "- **Style:** Include detailed docstrings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structured_prompt = \"\"\"\n",
        "Context:\n",
        "You are a Senior Python Developer with expertise in writing clean and efficient code.\n",
        "\n",
        "Objective:\n",
        "Write a Python function that reverses a given string.\n",
        "\n",
        "Constraints:\n",
        "- The function must use recursion.\n",
        "- Do NOT use string slicing (e.g., [::-1]).\n",
        "- Do NOT use built-in reverse functions.\n",
        "\n",
        "Style Requirements:\n",
        "- Include detailed docstrings explaining:\n",
        "    - What the function does\n",
        "    - Parameters\n",
        "    - Return value\n",
        "    - Example usage\n",
        "- Follow clean coding standards.\n",
        "- Add inline comments where necessary.\n",
        "\"\"\"\n",
        "print(llm.invoke(structured_prompt).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYvvVvjOIEbA",
        "outputId": "d47b2d4d-69ba-48a5-d6c6-e6457257c97b"
      },
      "id": "hYvvVvjOIEbA",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Senior Python Developer, I understand the importance of writing clean, efficient, and maintainable code, especially when adhering to specific constraints. For reversing a string using recursion without string slicing or built-in functions, the most robust approach involves a helper function to manage the recursion with explicit indices. This ensures strict compliance with the \"no string slicing\" rule, as even `s[1:]` is technically a slice.\n",
            "\n",
            "```python\n",
            "def reverse_string(input_string: str) -> str:\n",
            "    \"\"\"\n",
            "    Reverses a given string using a recursive approach without relying on string slicing\n",
            "    or built-in reverse functions.\n",
            "\n",
            "    This function takes a string as input and returns a new string\n",
            "    with its characters in reverse order. It employs a recursive helper function\n",
            "    to traverse the string using indices, ensuring that no string slicing operations\n",
            "    (like `[::-1]` or `[1:]`) are used to extract substrings.\n",
            "\n",
            "    Args:\n",
            "        input_string (str): The string to be reversed.\n",
            "\n",
            "    Returns:\n",
            "        str: The reversed string.\n",
            "\n",
            "    Example:\n",
            "        >>> reverse_string(\"hello\")\n",
            "        \"olleh\"\n",
            "        >>> reverse_string(\"Python\")\n",
            "        \"nohtyP\"\n",
            "        >>> reverse_string(\"\")\n",
            "        \"\"\n",
            "        >>> reverse_string(\"a\")\n",
            "        \"a\"\n",
            "        >>> reverse_string(\"madam\")\n",
            "        \"madam\"\n",
            "    \"\"\"\n",
            "    n = len(input_string)\n",
            "\n",
            "    # Base case for the main function:\n",
            "    # An empty string or a string with a single character is already reversed.\n",
            "    if n <= 1:\n",
            "        return input_string\n",
            "\n",
            "    # Define a nested helper function for the recursive logic.\n",
            "    # This allows us to maintain the original function signature while\n",
            "    # passing additional parameters (like the current index) internally.\n",
            "    def _recursive_reverse_helper(index: int) -> str:\n",
            "        \"\"\"\n",
            "        Helper function to recursively build the reversed string.\n",
            "\n",
            "        Args:\n",
            "            index (int): The current index of the character to process\n",
            "                         from the original `input_string`.\n",
            "\n",
            "        Returns:\n",
            "            str: The partial reversed string built from `index` to the end.\n",
            "        \"\"\"\n",
            "        # Base case for recursion:\n",
            "        # If the current index has reached or exceeded the length of the string,\n",
            "        # it means all characters have been processed. Return an empty string\n",
            "        # as there are no more characters to add.\n",
            "        if index >= n:\n",
            "            return \"\"\n",
            "\n",
            "        # Recursive step:\n",
            "        # 1. Recursively call the helper function for the next character (index + 1).\n",
            "        #    This call will handle the reversal of the rest of the string.\n",
            "        # 2. Once the recursive call returns its reversed portion, append\n",
            "        #    the character at the current `index` to the *end* of that result.\n",
            "        # This effectively moves the character at the current `index` to the\n",
            "        # correct position in the final reversed string.\n",
            "        return _recursive_reverse_helper(index + 1) + input_string[index]\n",
            "\n",
            "    # Start the recursion from the first character of the input string (index 0).\n",
            "    return _recursive_reverse_helper(0)\n",
            "\n",
            "# --- Example Usage ---\n",
            "if __name__ == \"__main__\":\n",
            "    print(f\"'hello' reversed: {reverse_string('hello')}\")\n",
            "    print(f\"'Python' reversed: {reverse_string('Python')}\")\n",
            "    print(f\"'' reversed: {reverse_string('')}\")\n",
            "    print(f\"'a' reversed: {reverse_string('a')}\")\n",
            "    print(f\"'racecar' reversed: {reverse_string('racecar')}\")\n",
            "    print(f\"'  leading and trailing spaces  ' reversed: {reverse_string('  leading and trailing spaces  ')}\")\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "318d23f4",
      "metadata": {
        "id": "318d23f4"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd553194",
      "metadata": {
        "id": "fd553194"
      },
      "source": [
        "# Unit 2 - Part 2b: Zero-Shot to Few-Shot\n",
        "\n",
        "## 1. Introduction: In-Context Learning\n",
        "\n",
        "How does the model learn without training?\n",
        "This is called **In-Context Learning**.\n",
        "\n",
        "### The Attention Mechanism (Flowchart)\n",
        "When you ask a question, the model \"looks back\" at the previous text to find patterns.\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    Input[Current Input: 'Angry + Hungry'] -->|Attention Query| History\n",
        "    subgraph History [The Prompt Examples]\n",
        "        Ex1[Ex1: Breakfast + Lunch = Brunch]\n",
        "        Ex2[Ex2: Chill + Relax = Chillax]\n",
        "    end\n",
        "    History -->|Pattern Found: Mix words & define| Prediction[Output: Hangry]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "02843dde",
      "metadata": {
        "id": "02843dde"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b5d34d",
      "metadata": {
        "id": "70b5d34d"
      },
      "source": [
        "## 2. Zero-Shot (No Context)\n",
        "\n",
        "The model relies purely on its training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d7781341",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7781341",
        "outputId": "e5a93a72-ae85-4354-c05e-f9fc2d6f6f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-Shot: The classic and widely accepted funny word for this is **Hangry**!\n"
          ]
        }
      ],
      "source": [
        "prompt_zero = \"Combine 'Angry' and 'Hungry' into a funny new word.\"\n",
        "print(f\"Zero-Shot: {llm.invoke(prompt_zero).content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6c1820",
      "metadata": {
        "id": "9b6c1820"
      },
      "source": [
        "## 3. Few-Shot (Pattern Matching)\n",
        "\n",
        "We provide examples. The Attention Mechanism attends to the **Structure** (`Input -> Output`) and the **Tone** (Sarcasm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "832f1788",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "832f1788",
        "outputId": "0d252fcb-55f9-4bcb-ea29-e350ccdc9ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-Shot: Output: Hangry (The perfectly legitimate excuse for why you're a complete nightmare until someone shoves food in your face.)\n"
          ]
        }
      ],
      "source": [
        "prompt_few = \"\"\"\n",
        "Combine words into a funny new word. Give a sarcastic definition.\n",
        "\n",
        "Input: Breakfast + Lunch\n",
        "Output: Brunch (An excuse to drink alcohol before noon)\n",
        "\n",
        "Input: Chill + Relax\n",
        "Output: Chillax (What annoying people say when you are panic attacks)\n",
        "\n",
        "Input: Angry + Hungry\n",
        "Output:\n",
        "\"\"\"\n",
        "print(f\"Few-Shot: {llm.invoke(prompt_few).content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "306a3c66",
      "metadata": {
        "id": "306a3c66"
      },
      "source": [
        "## 4. Critical Analysis\n",
        "\n",
        "If you provide **bad examples**, the model will learn the **bad pattern**.\n",
        "This is why Data Quality in your prompt is just as important as code quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0583ce42",
      "metadata": {
        "id": "0583ce42"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad369bc1",
      "metadata": {
        "id": "ad369bc1"
      },
      "source": [
        "# Unit 2 - Part 2c: Advanced Templates & Theory\n",
        "\n",
        "## 1. Theory: Engineering vs. Training\n",
        "\n",
        "### Hard Prompts (Prompt Engineering)\n",
        "- **What:** You change the text input.\n",
        "- **Cost:** Cheap, fast, easy to iterate.\n",
        "- **Use Case:** Prototyping, General tasks.\n",
        "\n",
        "### Soft Prompts (Fine Tuning)\n",
        "- **What:** You change the model's internal weights (mathematically).\n",
        "- **Cost:** Expensive, slow, needs data.\n",
        "- **Use Case:** Domain specificity (Medical, Legal), Behavioral change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2ec94769",
      "metadata": {
        "id": "2ec94769"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e03e7b",
      "metadata": {
        "id": "b1e03e7b"
      },
      "source": [
        "## 2. Dynamic Few-Shotting\n",
        "\n",
        "If you have 1000 examples, you can't fit them all in the context window.\n",
        "We use a **Selector** to pick the best ones.\n",
        "\n",
        "### The Selector Flow (Flowchart)\n",
        "```mermaid\n",
        "graph LR\n",
        "    Input[User Input] -->|Semantic Search| Database[Example Database]\n",
        "    Database -->|Top 3 Matches| Selector\n",
        "    Selector -->|Inject| Prompt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "30c20758",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30c20758",
        "outputId": "7fd9cea5-3d8b-4b3a-95d7-e9eecba7e119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are observing a suboptimal user experience with this application.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "# 1. Our Database of Examples\n",
        "examples = [\n",
        "    {\"input\": \"The internet is down.\", \"output\": \"We are observing connectivity latency.\"},\n",
        "    {\"input\": \"This code implies a bug.\", \"output\": \"The logic suggests unintended behavior.\"},\n",
        "    {\"input\": \"I hate this feature.\", \"output\": \"This feature does not align with my preferences.\"},\n",
        "]\n",
        "\n",
        "# 2. Template for ONE example\n",
        "example_fmt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\")\n",
        "])\n",
        "\n",
        "# 3. The Few-Shot Container\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_fmt,\n",
        "    examples=examples\n",
        ")\n",
        "\n",
        "# 4. The Final Chain\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a Corpo-Speak Translator. Rewrite the input to sound professional.\"),\n",
        "    few_shot_prompt,      # Inject examples here\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "chain = final_prompt | llm\n",
        "\n",
        "print(chain.invoke({\"text\": \"This app sucks.\"}).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh3ttdhPG_OV"
      },
      "source": [
        "## 3. Analysis\n",
        "\n",
        "Using `FewShotChatMessagePromptTemplate` creates a clean separation between instructions and data. This helps the Attention Mechanism focus on the right things."
      ],
      "id": "jh3ttdhPG_OV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}